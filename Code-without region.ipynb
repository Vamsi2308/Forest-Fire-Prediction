{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Analysis and getting to know the data better by generating the graphs  by cause of fire and which region is getting effected by the fire  mostly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wildfires.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_df = df[[\"stat_cause_descr\",\"latitude\",\"longitude\",\"state\",\"disc_clean_date\",\"fire_size\" ]]\n",
    "# print(subset_df.head())\n",
    "# subset_df['stat_cause_descr'].value_counts().plot(kind='bar',color='blue')\n",
    "# plt.xlabel(\"Cause of fires\")\n",
    "# plt.ylabel(\"Number of fires\")\n",
    "# plt.show()\n",
    "\n",
    "subset_df = df[[\"stat_cause_descr\", \"latitude\", \"longitude\", \"state\", \"disc_clean_date\", \"fire_size\"]]\n",
    "print(subset_df.head())\n",
    "subset_df['stat_cause_descr'].value_counts().plot(kind='bar', color='lightblue')\n",
    "plt.xlabel(\"Cause of fires\")\n",
    "plt.ylabel(\"Number of fires\")\n",
    "plt.title(\"Number of Fires by Cause\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['state'].value_counts().head(n=10).plot(kind='bar',color='lightblue')\n",
    "plt.title(\"Top states\")\n",
    "plt.ylabel(\"Number of fires\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CA = subset_df[subset_df['state']=='CA']\n",
    "df_GA = subset_df[subset_df['state']=='GA']\n",
    "df_TX = subset_df[subset_df['state']=='TX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GA['stat_cause_descr'].value_counts().plot(kind='bar',color='lightblue',title='causes of fires for GA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TX['stat_cause_descr'].value_counts().plot(kind='bar',color='lightblue',title='causes of fires for TX')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CA['stat_cause_descr'].value_counts().plot(kind='bar',color='lightblue',title='causes of fires for CA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_natural = subset_df[subset_df['stat_cause_descr']=='Lightning'].size\n",
    "df_artificial = subset_df[subset_df['stat_cause_descr']!='Lightning'].size\n",
    "df_misc = subset_df[subset_df['stat_cause_descr'] == 'Miscellaneous'].size\n",
    "df_misc+= subset_df[subset_df['stat_cause_descr'] == 'Missing/Undefined'].size\n",
    "# df_natural['stat_cause_descr'].value_counts().plot(kind='bar',color='forestgreen',title='causes of fires because of natural reasons')\n",
    "# df_artificial['stat_cause_descr'].value_counts().plot(kind='bar',color='forestgreen',title='causes of fires because of artificial reasons')\n",
    "data = {'Category':['Natural', 'Man made','Miscellaneous'],\n",
    "        'Values':[df_natural, df_artificial,df_misc]}\n",
    "df_1 = pd.DataFrame(data)\n",
    "plt.bar(df_1['Category'],df_1['Values'],color = 'lightblue')\n",
    "plt.show()\n",
    "# df_lightning.plot(x='state',color ='forestgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_df.plot(kind='scatter',x='longitude',y='latitude',color='coral',alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above are the key insights of how fires are in the US states and what is the main cause of the fires based on the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the Pre-processing steps for the preprocessing the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing and EDA(Exploratory data analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and filtering data which has firesize <5000 as number of small fires are high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the combined CSV files\n",
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1','disc_date_final','cont_date_final','cont_clean_date','putout_time'])\n",
    "df['disc_clean_date'] = pd.to_datetime(df['disc_clean_date'], format='%m/%d/%Y')\n",
    "\n",
    "#Get rid of outliers - fires of size larger than 5000 acres, and there are large number of small fires and other very less number are having the high \n",
    "# area of fires, because of which the deviation is very high\n",
    "df = df.loc[df['fire_size'] < 5000]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As these fieds are categorial, trying to change them into boolean using the one-hot encoding.\n",
    "# So that it will be easy to train.\n",
    "\n",
    "df['Vegetation'] = df['Vegetation'].astype('category')\n",
    "df['Cause'] = df['stat_cause_descr'].astype('category')\n",
    "\n",
    "df = pd.get_dummies(df,prefix=['Vegetation'], columns = ['Vegetation'], drop_first=True)\n",
    "df = pd.get_dummies(df,prefix=['Cause'], columns = ['stat_cause_descr'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerics_only = df.select_dtypes(include=np.number)\n",
    "\n",
    "corr = df_numerics_only.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(220, 20, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");\n",
    "sns.set(rc={'figure.figsize':(15,15)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with missing data\n",
    "print(len(df))\n",
    "\n",
    "# drop columns where weather_file is missing in the data, as it wont have the weather situation at that time, so its where ever data is \n",
    "#missing we can remove those rows as it wont be useful\n",
    "index = df[df['weather_file'] == 'File Not Found'].index\n",
    "df.drop(index, inplace = True)\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data has a lot of 0 and values some of which may be missing values,\n",
    "# Mark '0' values in weather columns as Na (to see how many there are) \n",
    "# As 0 wont add any value to the data, we are converting to NA and then removing them which will make data set\n",
    "subset0 = ['Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont']\n",
    "df[subset0] = df[subset0].replace({0:np.nan, '0':np.nan})\n",
    "print(len(df))\n",
    "\n",
    "# Mark '-1' as missing\n",
    "subset_neg1 = ['Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont']\n",
    "df[subset_neg1] = df[subset_neg1].replace({-1:np.nan})\n",
    "\n",
    "# Drop observations where all weather columns are 0\n",
    "df = df.dropna(how='all',\n",
    "                    subset=['Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont',])\n",
    "print(len(df))\n",
    "# This leaves us with 38,689 observations  +/- 3,000  to work with (originally we had 50,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the 'pre' columns temp wind and humidity with mean values\n",
    "subset_fill_mean = ['Temp_pre_30','Temp_pre_15','Temp_pre_7', 'Wind_pre_30','Wind_pre_15','Wind_pre_7', 'Hum_pre_30', 'Hum_pre_15','Hum_pre_7']\n",
    "df[subset_fill_mean] = df[subset_fill_mean].fillna(df[subset_fill_mean].mean())\n",
    "\n",
    "# Fill NAs in the date of fire containment based on mean values from previous days\n",
    "for col in ['Temp','Wind','Hum']:\n",
    "    df[f'{col}_cont'] = df.apply(\n",
    "        lambda row: (row[f'{col}_pre_7']+row[f'{col}_pre_15']+row[f'{col}_pre_30'])/3 if np.isnan(row[f'{col}_cont']) else row[f'{col}_cont'],\n",
    "        axis=1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separation of Experiments into 4 types - for better understanding of the effects on dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 - which will select all the available  features from the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 - which will select all teh available  features from the dataset\n",
    "#Features included - variables related to Vegetation,Temperature, Humidity, Wind, Precipitation, cause of  fire, longitude and latitude\n",
    "# we have 34 variables  for x-variables  to which we are gonna target one y-variable which is fire_size\n",
    "# selecting features and target variables\n",
    "X1 = df[['Vegetation_4','remoteness', 'Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "# X1 = df[['Vegetation_4','Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "\n",
    "y = df['fire_size']\n",
    "\n",
    "#train test split\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "df1 = [X1_train, X1_test, y_train, y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment  type 2 :  Include only long, lat, vegetation, cause and pre- weather data, without cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is the data set where I removed the variables  on which the fire is  containining on the day\n",
    "# removed 4 variables\n",
    "# selecting features and target variables\n",
    "X2 = df[['Vegetation_4','Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Wind_pre_30','Wind_pre_15','Wind_pre_7','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Prec_pre_30','Prec_pre_15','Prec_pre_7','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "y = df['fire_size']\n",
    "\n",
    "#train test split\n",
    "X2_train, X2_test, y_train, y_test = train_test_split(X2, y, test_size=0.2, random_state=42)\n",
    "df2 = [X2_train, X2_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 - Including only lat, long and weather pre- data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When I have done the feature importance, I got to know that the cause and vegetation is not that important, so here we removed the 2 \n",
    "#  selecting features and target variables\n",
    "X3 = df[['latitude','longitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Wind_pre_30','Wind_pre_15','Wind_pre_7','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Prec_pre_30','Prec_pre_15','Prec_pre_7']]\n",
    "y = df['fire_size']\n",
    "\n",
    "#train test split\n",
    "X3_train, X3_test, y_train, y_test = train_test_split(X3, y, test_size=0.2, random_state=42)\n",
    "df3 = [X3_train, X3_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4 :-  with experiment 1 data with normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have done the minMax normalization for the experiment 1 data frame.\n",
    "\n",
    "df_4 = df[['Vegetation_4','remoteness','Vegetation_9','Vegetation_12','Vegetation_14','Vegetation_15','Vegetation_16','latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','Cause_Debris Burning','Cause_Equipment Use','Cause_Fireworks','Cause_Lightning','Cause_Miscellaneous','Cause_Missing/Undefined','Cause_Powerline','Cause_Railroad','Cause_Smoking','Cause_Structure', 'longitude']]\n",
    "names = df_4.columns\n",
    "\n",
    "# normalizing data\n",
    "df_4 = preprocessing.normalize(df_4)\n",
    "scaled_df = pd.DataFrame(df_4, columns=names)\n",
    "\n",
    "#train test split\n",
    "X4_train, X4_test, y_train, y_test = train_test_split(scaled_df, y, test_size=0.2, random_state=42)\n",
    "df4 = [X4_train, X4_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying different Models: \n",
    "### The above experiments with different models like  decision tree,gradient bosting, random  forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree - Experiment 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectr = DecisionTreeRegressor(random_state=0)\n",
    "dectr.fit(df1[0], df1[2])\n",
    "\n",
    "predictions = dectr.predict(df1[1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(df1[3], predictions))\n",
    "print('R Squared:', metrics.r2_score(df1[3], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Gradient Boosting - Experiment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_boost = GradientBoostingRegressor()\n",
    "gr_boost.fit(df1[0], df1[2])\n",
    "\n",
    "predictions = gr_boost.predict(df1[1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(df1[3], predictions))\n",
    "print('R Squared:', metrics.r2_score(df1[3], predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(df1[0], df1[2])\n",
    "\n",
    "predictions = rf.predict(df1[1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(df1[3], predictions))\n",
    "print('R Squared:', metrics.r2_score(df1[3], predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling all the experirments by printing the results at one place\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than 5000\n",
    "for model in [DecisionTreeRegressor(), GradientBoostingRegressor(), RandomForestRegressor()]:\n",
    "    for idx,df in enumerate([df1, df2, df3, df4]):\n",
    "        model.fit(df[0], df[2])\n",
    "        print(f'{model}; Experiment {idx+1}; Mean Absolute Error:', metrics.mean_absolute_error(df[3], model.predict(df[1])))\n",
    "        print(f'{model}; Experiment {idx+1}; R Squared:', metrics.r2_score(df[3], model.predict(df[1])))\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results :\n",
    "The best performing basemodel is the Random Forest algorithm with Experiment 1. This is the model we will use for further analysis and improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "What features are the most influential in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "    index = df1[0].columns,columns=['importance']).sort_values('importance',ascending=False)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor()\n",
    "search_grid={'n_estimators':[50,100,200],'max_depth':[2,5,8,10]}\n",
    "search=GridSearchCV(estimator=rf_reg,param_grid=search_grid,scoring='neg_mean_absolute_error',n_jobs=1,cv=5, verbose=1)\n",
    "search.fit(df1[0], df1[2])\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor \n",
    "rf_reg = RandomForestRegressor(n_estimators = 200, max_depth=10)\n",
    "\n",
    "# fit the regressor with x and y data\n",
    "rf_reg.fit(df1[0], df1[2])\n",
    "\n",
    "predictions = rf_reg.predict(df1[1])\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(df1[3], predictions))\n",
    "print('R Squared:', metrics.r2_score(df1[3], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['latitude','Temp_pre_30','Temp_pre_15','Temp_pre_7','Temp_cont','Wind_pre_30','Wind_pre_15','Wind_pre_7','Wind_cont','Hum_pre_30', 'Hum_pre_15','Hum_pre_7','Hum_cont','Prec_pre_30','Prec_pre_15','Prec_pre_7','Prec_cont','longitude']]\n",
    "\n",
    "y = df['fire_size']\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neural network model and its results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #normalizer,\n",
    "    Dense(34, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01), kernel_initializer='normal',input_dim = X_train.shape[1]),\n",
    "    Dense(34, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01)),\n",
    "    Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01)),\n",
    "    Dense(32, activation=\"relu\", kernel_regularizer = regularizers.l2(0.01)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['accuracy','mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    validation_split=0.15,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_mae',\n",
    "        patience=20,\n",
    "        mode='min',\n",
    "        restore_best_weights=True\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "historydf = pd.DataFrame(history.history)\n",
    "\n",
    "#Run this cell to plot the epoch vs loss graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(historydf['mae'],label='mae')\n",
    "plt.plot(historydf['val_mae'],label='val_mae')\n",
    "plt.title('MAE vs. epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_mae = model.evaluate(x=X_test, y=y_test, verbose=0)\n",
    "print('Mean Absolute Error: {acc:0.3f}'.format(acc=test_mae))\n",
    "print('accuracy: {acc:0.3f}'.format(acc=test_acc))\n",
    "print('loss: {acc:0.3f}'.format(acc=test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Introduction:\n",
    "\n",
    "Starting the classification process, and checking if we need to do more preprocessing steps to the current dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis='rows', how='any', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size=0.30, random_state=568)\n",
    "\n",
    "#Train set -Int64Index: 38756 entries, 16778 to 41212\n",
    "print(train_set.info())\n",
    "# Test Set - Int64Index: 16611 entries, 45469 to 21122\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasons and Insights for selecting the feature list\n",
    "\n",
    "I am dividing the feature list into 2 parts as we have already done the feature importance.\n",
    "\n",
    "Why I am selecting features, as we have seen before most fires are based on the natural cause, i want to predict the  based on the climatic conditions\n",
    "\n",
    "### List 1\n",
    "--> X = \"remoteness\",\"latitude\" ,\"longitude\",\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"\n",
    "\n",
    "### List 2\n",
    "--> X = \"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"\n",
    "\n",
    "### Target variables\n",
    "Y = \"fire_size_class\" || \"fire_size\"\n",
    "\n",
    "## issues while selecting the targets  \n",
    "\n",
    "When I was using \"fire_size_class\" for the Y feature, I had gotten continuous value error. After that using label encoder changed that unknown label error  was fixed and able to predict for the target  variable fire_size too.\n",
    "\n",
    "## Reasons for selecting the target variable fire_size and fire_size_class\n",
    "\n",
    "As our project end goal is to predict the fire_size, which will be useful for the people in real time to get to know the size of the fire and can be evacuated to the safe plce. But while working on the project, we felt fire_size_class is important as the fire_size that will give us the reason of the fire.\n",
    "\n",
    "Also from the results of the PCA and Feature importance, we have finalised these target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = train_set[[\"remoteness\",\"latitude\" ,\"longitude\",\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "#X = train_set[[\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "\n",
    "Y = train_set[\"fire_size_class\"]\n",
    "# Y = train_set[\"fire_size\"]\n",
    "\n",
    "#Transforming the \"unknown lable continuous variable \"error into label encoder and applying for the classification\n",
    "\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(Y)\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier().fit(X,y_transformed)\n",
    "print(tree_classifier)\n",
    "\n",
    "y_pred = tree_classifier.predict(X)\n",
    "c_matrix = confusion_matrix(y_transformed, y_pred)\n",
    "print('The confusion Matrix is: ')\n",
    "print(c_matrix)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "print(\"The Evalution metrics are as follows: \")\n",
    "print(\"Accuracy: \", accuracy_score(y_transformed, y_pred))\n",
    "print(\"Precision: \", precision_score(y_transformed, y_pred, average=\"weighted\"))\n",
    "print(\"Sensitivity: \", recall_score(y_transformed, y_pred, average=\"weighted\"))\n",
    "print(\"F1 Score: \", f1_score(y_transformed, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gaussian Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Training Set\n",
    "\n",
    "X = train_set[[\"remoteness\",\"latitude\" ,\"longitude\",\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "#X = train_set[[\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "Y = train_set[\"fire_size_class\"]\n",
    "# Y = train_set[\"fire_size\"]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "print(gnb.fit(X,Y))\n",
    "\n",
    "y_pred = gnb.predict(X)\n",
    "c_matrix = confusion_matrix(Y, y_pred)\n",
    "print(\"Confusion Metrix: \")\n",
    "print(c_matrix)\n",
    "\n",
    "print(\"The Evalution metrics are as follows: \")\n",
    "print(\"Accuracy: \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision: \", precision_score(Y, y_pred, average=\"weighted\"))\n",
    "print(\"Sensitivity: \", recall_score(Y, y_pred, average=\"weighted\"))\n",
    "print(\"F1 Score: \", f1_score(Y, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = train_set[[\"remoteness\",\"latitude\" ,\"longitude\",\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "#X = train_set[[\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "Y = train_set[\"fire_size_class\"]\n",
    "# Y = train_set[\"fire_size\"]\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "print(rfc.fit(X,Y))\n",
    "\n",
    "y_pred = rfc.predict(X)\n",
    "c_matrix = confusion_matrix(Y, y_pred)\n",
    "print(\"Confusion Metrix: \")\n",
    "print(c_matrix)\n",
    "\n",
    "print(\"The Evalution metrics are as follows: \")\n",
    "print(\"Accuracy: \", accuracy_score(Y, y_pred))\n",
    "print(\"Precision: \", precision_score(Y, y_pred, average=\"weighted\"))\n",
    "print(\"Sensitivity: \", recall_score(Y, y_pred, average=\"weighted\"))\n",
    "print(\"F1 Score: \", f1_score(Y, y_pred, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solved the issue with fire_size\n",
    "A code sample for the fire size  as a target, Removed the \"Unknown label continuous variable  error\" as its not categorical data for the fire_size. \n",
    "\n",
    "Transformed that column using label encoder and transformed, so that we can apply the different classification function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the same code for the Fire_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = train_set[[\"remoteness\",\"latitude\" ,\"longitude\",\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "#X = train_set[[\"Temp_pre_7\",\"Wind_pre_7\",\"Hum_pre_7\",\"Prec_pre_7\"]]\n",
    "\n",
    "Y = train_set[\"fire_size\"]\n",
    "\n",
    "#Transforming the \"unknown lable continuous variable \"error into label encoder and applying for the classification\n",
    "\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(Y)\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier().fit(X,y_transformed)\n",
    "print(tree_classifier)\n",
    "\n",
    "y_pred = tree_classifier.predict(X)\n",
    "c_matrix = confusion_matrix(y_transformed, y_pred)\n",
    "print('The confusion Matrix is: ')\n",
    "print(c_matrix)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score, f1_score\n",
    "print(\"The Evalution metrics are as follows: \")\n",
    "print(\"Accuracy: \", accuracy_score(y_transformed, y_pred))\n",
    "print(\"Precision: \", precision_score(y_transformed, y_pred, average=\"weighted\"))\n",
    "print(\"Sensitivity: \", recall_score(y_transformed, y_pred, average=\"weighted\"))\n",
    "print(\"F1 Score: \", f1_score(y_transformed, y_pred, average=\"weighted\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
